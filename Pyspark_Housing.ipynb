{"nbformat_minor": 2, "cells": [{"execution_count": 33, "cell_type": "code", "source": "rdd = sc.textFile('wasb:///example/data/cal_housing.data')\nheader = sc.textFile('wasb:///example/data/cal_housing.domain')", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 34, "cell_type": "code", "source": "header.collect()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "['longitude: continuous.', 'latitude: continuous.', 'housingMedianAge: continuous. ', 'totalRooms: continuous. ', 'totalBedrooms: continuous. ', 'population: continuous. ', 'households: continuous. ', 'medianIncome: continuous. ', 'medianHouseValue: continuous. ']"}], "metadata": {"collapsed": false}}, {"execution_count": 35, "cell_type": "code", "source": "rdd.take(1)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "['-122.230000,37.880000,41.000000,880.000000,129.000000,322.000000,126.000000,8.325200,452600.000000']"}], "metadata": {"collapsed": false}}, {"execution_count": 36, "cell_type": "code", "source": "rdd = rdd.map(lambda line: line.split(\",\"))", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 38, "cell_type": "code", "source": "rdd.take(1)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[['-122.230000', '37.880000', '41.000000', '880.000000', '129.000000', '322.000000', '126.000000', '8.325200', '452600.000000']]"}], "metadata": {"collapsed": false}}, {"execution_count": 39, "cell_type": "code", "source": "# Import the necessary modules \nfrom pyspark.sql import Row\n\n# Map the RDD to a DF\ndf = rdd.map(lambda line: Row(longitude=line[0], \n                              latitude=line[1], \n                              housingMedianAge=line[2],\n                              totalRooms=line[3],\n                              totalBedRooms=line[4],\n                              population=line[5], \n                              households=line[6],\n                              medianIncome=line[7],\n                              medianHouseValue=line[8])).toDF()", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 40, "cell_type": "code", "source": "# Show the top 20 rows \ndf.show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-----------+----------------+---------+-----------+----------------+------------+-----------+-------------+-----------+\n| households|housingMedianAge| latitude|  longitude|medianHouseValue|medianIncome| population|totalBedRooms| totalRooms|\n+-----------+----------------+---------+-----------+----------------+------------+-----------+-------------+-----------+\n| 126.000000|       41.000000|37.880000|-122.230000|   452600.000000|    8.325200| 322.000000|   129.000000| 880.000000|\n|1138.000000|       21.000000|37.860000|-122.220000|   358500.000000|    8.301400|2401.000000|  1106.000000|7099.000000|\n| 177.000000|       52.000000|37.850000|-122.240000|   352100.000000|    7.257400| 496.000000|   190.000000|1467.000000|\n| 219.000000|       52.000000|37.850000|-122.250000|   341300.000000|    5.643100| 558.000000|   235.000000|1274.000000|\n| 259.000000|       52.000000|37.850000|-122.250000|   342200.000000|    3.846200| 565.000000|   280.000000|1627.000000|\n| 193.000000|       52.000000|37.850000|-122.250000|   269700.000000|    4.036800| 413.000000|   213.000000| 919.000000|\n| 514.000000|       52.000000|37.840000|-122.250000|   299200.000000|    3.659100|1094.000000|   489.000000|2535.000000|\n| 647.000000|       52.000000|37.840000|-122.250000|   241400.000000|    3.120000|1157.000000|   687.000000|3104.000000|\n| 595.000000|       42.000000|37.840000|-122.260000|   226700.000000|    2.080400|1206.000000|   665.000000|2555.000000|\n| 714.000000|       52.000000|37.840000|-122.250000|   261100.000000|    3.691200|1551.000000|   707.000000|3549.000000|\n| 402.000000|       52.000000|37.850000|-122.260000|   281500.000000|    3.203100| 910.000000|   434.000000|2202.000000|\n| 734.000000|       52.000000|37.850000|-122.260000|   241800.000000|    3.270500|1504.000000|   752.000000|3503.000000|\n| 468.000000|       52.000000|37.850000|-122.260000|   213500.000000|    3.075000|1098.000000|   474.000000|2491.000000|\n| 174.000000|       52.000000|37.840000|-122.260000|   191300.000000|    2.673600| 345.000000|   191.000000| 696.000000|\n| 620.000000|       52.000000|37.850000|-122.260000|   159200.000000|    1.916700|1212.000000|   626.000000|2643.000000|\n| 264.000000|       50.000000|37.850000|-122.260000|   140000.000000|    2.125000| 697.000000|   283.000000|1120.000000|\n| 331.000000|       52.000000|37.850000|-122.270000|   152500.000000|    2.775000| 793.000000|   347.000000|1966.000000|\n| 303.000000|       52.000000|37.850000|-122.270000|   155500.000000|    2.120200| 648.000000|   293.000000|1228.000000|\n| 419.000000|       50.000000|37.840000|-122.260000|   158700.000000|    1.991100| 990.000000|   455.000000|2239.000000|\n| 275.000000|       52.000000|37.840000|-122.270000|   162900.000000|    2.603300| 690.000000|   298.000000|1503.000000|\n+-----------+----------------+---------+-----------+----------------+------------+-----------+-------------+-----------+\nonly showing top 20 rows"}], "metadata": {"collapsed": false}}, {"execution_count": 42, "cell_type": "code", "source": "df.columns", "outputs": [{"output_type": "stream", "name": "stdout", "text": "['households', 'housingMedianAge', 'latitude', 'longitude', 'medianHouseValue', 'medianIncome', 'population', 'totalBedRooms', 'totalRooms']"}], "metadata": {"collapsed": false}}, {"execution_count": 43, "cell_type": "code", "source": "df.dtypes", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[('households', 'string'), ('housingMedianAge', 'string'), ('latitude', 'string'), ('longitude', 'string'), ('medianHouseValue', 'string'), ('medianIncome', 'string'), ('population', 'string'), ('totalBedRooms', 'string'), ('totalRooms', 'string')]"}], "metadata": {"collapsed": false}}, {"execution_count": 44, "cell_type": "code", "source": "df.printSchema()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "root\n |-- households: string (nullable = true)\n |-- housingMedianAge: string (nullable = true)\n |-- latitude: string (nullable = true)\n |-- longitude: string (nullable = true)\n |-- medianHouseValue: string (nullable = true)\n |-- medianIncome: string (nullable = true)\n |-- population: string (nullable = true)\n |-- totalBedRooms: string (nullable = true)\n |-- totalRooms: string (nullable = true)"}], "metadata": {"collapsed": false}}, {"execution_count": 49, "cell_type": "code", "source": "# Import all from `sql.types`\nfrom pyspark.sql.types import *\n\n# Write a custom function to convert the data type of DataFrame columns\ndef convertColumn(df, names, newType):\n  for name in names: \n     df = df.withColumn(name, df[name].cast(newType))\n  return df \n\n# Assign all column names to `columns`\ncolumns = ['households', 'housingMedianAge', 'latitude', 'longitude', 'medianHouseValue', 'medianIncome', 'population', 'totalBedRooms', 'totalRooms']\n\n# Conver the `df` columns to `FloatType()`\ndf = convertColumn(df, columns, FloatType())", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 50, "cell_type": "code", "source": "df.printSchema()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "root\n |-- households: float (nullable = true)\n |-- housingMedianAge: float (nullable = true)\n |-- latitude: float (nullable = true)\n |-- longitude: float (nullable = true)\n |-- medianHouseValue: float (nullable = true)\n |-- medianIncome: float (nullable = true)\n |-- population: float (nullable = true)\n |-- totalBedRooms: float (nullable = true)\n |-- totalRooms: float (nullable = true)"}], "metadata": {"collapsed": false}}, {"execution_count": 51, "cell_type": "code", "source": "df.select('population','totalBedRooms').show(10)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+----------+-------------+\n|population|totalBedRooms|\n+----------+-------------+\n|     322.0|        129.0|\n|    2401.0|       1106.0|\n|     496.0|        190.0|\n|     558.0|        235.0|\n|     565.0|        280.0|\n|     413.0|        213.0|\n|    1094.0|        489.0|\n|    1157.0|        687.0|\n|    1206.0|        665.0|\n|    1551.0|        707.0|\n+----------+-------------+\nonly showing top 10 rows"}], "metadata": {"collapsed": false}}, {"execution_count": 53, "cell_type": "code", "source": "df.groupBy('housingMedianAge').count().sort('housingMedianAge', ascending=False).show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+----------------+-----+\n|housingMedianAge|count|\n+----------------+-----+\n|            52.0| 1273|\n|            51.0|   48|\n|            50.0|  136|\n|            49.0|  134|\n|            48.0|  177|\n|            47.0|  198|\n|            46.0|  245|\n|            45.0|  294|\n|            44.0|  356|\n|            43.0|  353|\n|            42.0|  368|\n|            41.0|  296|\n|            40.0|  304|\n|            39.0|  369|\n|            38.0|  394|\n|            37.0|  537|\n|            36.0|  862|\n|            35.0|  824|\n|            34.0|  689|\n|            33.0|  615|\n+----------------+-----+\nonly showing top 20 rows"}], "metadata": {"collapsed": false}}, {"execution_count": 56, "cell_type": "code", "source": "df.describe().show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+-----------------+------------------+\n|summary|        households|  housingMedianAge|          latitude|          longitude|  medianHouseValue|      medianIncome|        population|    totalBedRooms|        totalRooms|\n+-------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+-----------------+------------------+\n|  count|             20640|             20640|             20640|              20640|             20640|             20640|             20640|            20640|             20640|\n|   mean| 499.5396802325581|28.639486434108527| 35.63186143109965|-119.56970444871473|206855.81690891474|3.8706710030346416|1425.4767441860465|537.8980135658915|2635.7630813953488|\n| stddev|382.32975283161136|12.585557612111613|2.1359523806029554|  2.003531742932892|115395.61587441381|1.8998217183639672|1132.4621217653385|421.2479059431315|   2181.6152515828|\n|    min|               1.0|               1.0|             32.54|            -124.35|           14999.0|            0.4999|               3.0|              1.0|               2.0|\n|    max|            6082.0|              52.0|             41.95|            -114.31|          500001.0|           15.0001|           35682.0|           6445.0|           39320.0|\n+-------+------------------+------------------+------------------+-------------------+------------------+------------------+------------------+-----------------+------------------+"}], "metadata": {"collapsed": false}}, {"execution_count": 57, "cell_type": "code", "source": "# Import all from `sql.functions` \nfrom pyspark.sql.functions import *\n\n# Adjust the values of `medianHouseValue`\ndf = df.withColumn(\"medianHouseValue\", col(\"medianHouseValue\")/100000)\n\n# Show the first 2 lines of `df`\ndf.take(2)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[Row(households=126.0, housingMedianAge=41.0, latitude=37.880001068115234, longitude=-122.2300033569336, medianHouseValue=4.526, medianIncome=8.325200080871582, population=322.0, totalBedRooms=129.0, totalRooms=880.0), Row(households=1138.0, housingMedianAge=21.0, latitude=37.86000061035156, longitude=-122.22000122070312, medianHouseValue=3.585, medianIncome=8.301400184631348, population=2401.0, totalBedRooms=1106.0, totalRooms=7099.0)]"}], "metadata": {"collapsed": false}}, {"execution_count": 58, "cell_type": "code", "source": "# Import all from `sql.functions` if you haven't yet\nfrom pyspark.sql.functions import *\n\n# Divide `totalRooms` by `households`\nroomsPerHousehold = df.select(col(\"totalRooms\")/col(\"households\"))\n\n# Divide `population` by `households`\npopulationPerHousehold = df.select(col(\"population\")/col(\"households\"))\n\n# Divide `totalBedRooms` by `totalRooms`\nbedroomsPerRoom = df.select(col(\"totalBedRooms\")/col(\"totalRooms\"))\n\n# Add the new columns to `df`\ndf = df.withColumn(\"roomsPerHousehold\", col(\"totalRooms\")/col(\"households\"))\\\n   .withColumn(\"populationPerHousehold\", col(\"population\")/col(\"households\"))\\\n   .withColumn(\"bedroomsPerRoom\", col(\"totalBedRooms\")/col(\"totalRooms\"))\n   \n# Inspect the result\ndf.first()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Row(households=126.0, housingMedianAge=41.0, latitude=37.880001068115234, longitude=-122.2300033569336, medianHouseValue=4.526, medianIncome=8.325200080871582, population=322.0, totalBedRooms=129.0, totalRooms=880.0, roomsPerHousehold=6.984126984126984, populationPerHousehold=2.5555555555555554, bedroomsPerRoom=0.14659090909090908)"}], "metadata": {"collapsed": false}}, {"execution_count": 59, "cell_type": "code", "source": "# Re-order and select columns\ndf = df.select(\"medianHouseValue\", \n              \"totalBedRooms\", \n              \"population\", \n              \"households\", \n              \"medianIncome\", \n              \"roomsPerHousehold\", \n              \"populationPerHousehold\", \n              \"bedroomsPerRoom\")", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 60, "cell_type": "code", "source": "df.take(1)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[Row(medianHouseValue=4.526, totalBedRooms=129.0, population=322.0, households=126.0, medianIncome=8.325200080871582, roomsPerHousehold=6.984126984126984, populationPerHousehold=2.5555555555555554, bedroomsPerRoom=0.14659090909090908)]"}], "metadata": {"collapsed": false}}, {"execution_count": 61, "cell_type": "code", "source": "# Import `DenseVector`\nfrom pyspark.ml.linalg import DenseVector\n\n# Define the `input_data` \ninput_data = df.rdd.map(lambda x: (x[0], DenseVector(x[1:])))\n\n# Replace `df` with the new DataFrame\ndf = spark.createDataFrame(input_data, [\"label\", \"features\"])", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 62, "cell_type": "code", "source": "# Import `StandardScaler` \nfrom pyspark.ml.feature import StandardScaler\n\n# Initialize the `standardScaler`\nstandardScaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\")\n\n# Fit the DataFrame to the scaler\nscaler = standardScaler.fit(df)\n\n# Transform the data in `df` with the scaler\nscaled_df = scaler.transform(df)\n\n# Inspect the result\nscaled_df.take(2)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[Row(label=4.526, features=DenseVector([129.0, 322.0, 126.0, 8.3252, 6.9841, 2.5556, 0.1466]), features_scaled=DenseVector([0.3062, 0.2843, 0.3296, 4.3821, 2.8228, 0.2461, 2.5264])), Row(label=3.585, features=DenseVector([1106.0, 2401.0, 1138.0, 8.3014, 6.2381, 2.1098, 0.1558]), features_scaled=DenseVector([2.6255, 2.1202, 2.9765, 4.3696, 2.5213, 0.2031, 2.6851]))]"}], "metadata": {"collapsed": false}}, {"execution_count": 63, "cell_type": "code", "source": "# Split the data into train and test sets\ntrain_data, test_data = scaled_df.randomSplit([.8,.2],seed=1234)", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 64, "cell_type": "code", "source": "# Import `LinearRegression`\nfrom pyspark.ml.regression import LinearRegression\n\n# Initialize `lr`\nlr = LinearRegression(labelCol=\"label\", maxIter=10, regParam=0.3, elasticNetParam=0.8)\n\n# Fit the data to the model\nlinearModel = lr.fit(train_data)", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 65, "cell_type": "code", "source": "# Generate predictions\npredicted = linearModel.transform(test_data)\n\n# Extract the predictions and the \"known\" correct labels\npredictions = predicted.select(\"prediction\").rdd.map(lambda x: x[0])\nlabels = predicted.select(\"label\").rdd.map(lambda x: x[0])\n\n# Zip `predictions` and `labels` into a list\npredictionAndLabel = predictions.zip(labels).collect()\n\n# Print out first 5 instances of `predictionAndLabel` \npredictionAndLabel[:5]", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[(1.4491508524918457, 0.14999), (1.5705029404692372, 0.14999), (2.148727956912464, 0.14999), (1.5831547768979277, 0.344), (1.5182107797955968, 0.398)]"}], "metadata": {"collapsed": false}}, {"execution_count": 69, "cell_type": "code", "source": "# Coefficients for the model\nlinearModel.coefficients\n\n# Intercept for the model\nlinearModel.intercept", "outputs": [{"output_type": "stream", "name": "stdout", "text": "0.9903995774620005"}], "metadata": {"collapsed": false}}, {"execution_count": 73, "cell_type": "code", "source": "# Get the RMSE\nlinearModel.summary.rootMeanSquaredError\n\n# Get the R2\nlinearModel.summary.r2", "outputs": [{"output_type": "stream", "name": "stdout", "text": "0.4240895287218379"}], "metadata": {"collapsed": false}}, {"execution_count": 74, "cell_type": "code", "source": "spark.stop()", "outputs": [], "metadata": {"collapsed": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "PySpark3", "name": "pyspark3kernel", "language": ""}, "language_info": {"mimetype": "text/x-python", "pygments_lexer": "python3", "name": "pyspark3", "codemirror_mode": {"version": 3, "name": "python"}}}}